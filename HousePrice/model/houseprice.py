# -*- coding: utf-8 -*-
"""HousePrice.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1g4tYdDFaW7jd9PxL7X0_IOYdWSb2aWfP
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
plt.rcParams["figure.figsize"]=(20, 10)

df = pd.read_csv("BengaluruDataset.csv")

df.head()

# Data Cleaning
df.shape
df.info()

df.groupby('area_type')['area_type'].agg('count')

df2 = df.drop(["area_type", "society", "balcony", "availability"], axis=1)
df2.head()

# checking null values
df2.isnull().sum()

# dropping NA due to small number of NA values present
df3 = df2.dropna()
df3.isnull().sum()

"""## **Data Cleaning**"""

df3['size'].unique()

# we simply split the values and consider pnly the integer part of the string

df3['bhk'] = df3['size'].apply(lambda x: int(x.split(" ")[0]))

df3.reset_index(inplace=True, drop=True)

df3['bhk'].unique()

# we can see that we got a range of values as one of the values of 'total_sqft'
df3.total_sqft.unique()

# determining whether a value is float or not

def is_float(x):
  try:
    float(x)
  except:
    return False
  return True

df3[~df3['total_sqft'].apply(is_float)].head(10)

# converting Perch to sqft

df3.loc[df3['total_sqft'].str.find("Perch")!=-1, 'total_sqft']= str(4125*272.25)

# converting sq meter to sqft

df3.loc[df3['total_sqft'].str.find("Sq. Meter")!=-1, 'total_sqft'] = df3.loc[df3['total_sqft'].str.find("Sq. Meter")!=-1, 'total_sqft'].apply(lambda x: str(float(x.split('Sq. Meter')[0])*10.7639))

# converting range vals in "total_sqft" to continuous value

def convert_to_num(x):
  tokens = x.split("-")
  if len(tokens)==2:
    return (float(tokens[0])+float(tokens[1]))/2
  try:
    return float(x)
  except:
    return None

df4 = df3.copy()

df4['total_sqft'] = df4['total_sqft'].apply(convert_to_num)
df4.head(10)

# so we finally handled the 'total_sqft column
df4[~df4['total_sqft'].apply(is_float)].head(10)

"""## **Feature Engineering**"""

df5 = df4.copy()
df5['price_per_sqft'] = df5['price']*100000/df5['total_sqft']
df5.head()

"""### We can see that we have a total of 1304 categorical(nominal) variable values and if we try to encode it, we will get many columns or features.
### This is called **dimensionality curse**
### We have techniques to reduce the dimensions **(Dimensionality Reduction)**. One of the effective techniques is to come up with other categories i.e. there maybe some catergorical values with few data-points
"""

len(df5.location.unique())

"""We can see that we have a maximum of 535 loactions for "WhiteField" and many data-points have only 1 data-point"""

df5.location = df5.location.apply(lambda x: x.strip())

# finding the categorical value with few data-points
location_stats = df5.groupby('location')['location'].agg('count').sort_values(ascending=False)
location_stats

"""We can confirm that there are 1052 locations with less than 10 data-points"""

location_stats_less_10 = location_stats[location_stats<=10]

len(location_stats_less_10)

"""So now we can put those categories in "others""""

df5.location = df5.location.apply(lambda x: 'other' if x in location_stats_less_10 else x)

len(df5.location.unique())

"""## **Outlier Detection & Removal**

We look whether sq. ft. per bedroom is less than certain threshold. Here we consider, let's say 300 sqft/bedroom
"""

df5[df5.total_sqft/df5.bhk<300].head()

df6 = df5[~(df5.total_sqft/df5.bhk<300)]
df6.shape

"""We check price/sq.ft per location for either very very high or very very low values"""

df6.price_per_sqft.describe()

"""removing price/sqft per location outliers
per location we find mean and std. deviation and filter data-points beyond 1 standard deviation (under 68%) (considering we have normal distribution)
"""

def remove_pps_outliers(df):
  df_out = pd.DataFrame()
  for key, subdf in df.groupby('location'):
    m = np.mean(subdf.price_per_sqft)
    st = np.std(subdf.price_per_sqft)
    reduced_df = subdf[(subdf.price_per_sqft>(m-st)) & (subdf.price_per_sqft<=(m+st))]
    df_out = pd.concat([df_out, reduced_df], ignore_index=True)
  return df_out

df7 = remove_pps_outliers(df6)
df7.shape

"""When we visualize different bhk prices in various locations, we can catch many outliers"""

def scatter_plot(df, location):
  # 2 BHK flat at particular location
  bhk2 = df[(df.location==location) & (df.bhk==2)]
  # 3 BHK flat at particular loaction
  bhk3 = df[(df.location==location) & (df.bhk==3)]
  plt.rcParams['figure.figsize']=(10, 6)
  plt.scatter(bhk2.total_sqft, bhk2.price, color="blue", label="2BHK", s=50)
  plt.scatter(bhk3.total_sqft, bhk3.price, color="red", label="3BHK", marker='+', s=50)
  plt.ylabel("Price")
  plt.xlabel("Total Sqaure feet area")
  plt.title(location)
  plt.legend()

scatter_plot(df7, 'Rajaji Nagar')

"""**We should remove properties where for same location, the price of (for example) 3 bedroom apartment is less than 2 bedroom apartment(with same square ft area). What we will do is for a given loaction, we will build a dictionary of stats per bhk, i.e.**

{
  '1' : {
    'mean': ,
    'std': ,
    'count': 
  },
  '2' : {
    'mean': ,
    'std': ,
    'count': 
  }
}

**Now we can remove those 2 BHK apartment whose price_per_sqft is less than mean price_per_sqft of 1 BHK apartment bold text**
"""

def remove_bhk_outliers(df):
  exclude_indices = np.array([])
  for location, location_df in df.groupby('location'):
    bhk_stats={}
    for bhk, bhk_df in location_df.groupby('bhk'):
      bhk_stats[bhk]={
          'mean': np.mean(bhk_df.price_per_sqft),
          'std': np.std(bhk_df.price_per_sqft),
          'count': bhk_df.shape[0]
      }
    for bhk, bhk_df in location_df.groupby('bhk'):
      stats = bhk_stats.get(bhk-1)
      if stats and stats['count']>5:
        exclude_indices = np.append(exclude_indices, bhk_df[bhk_df.price_per_sqft<(stats['mean'])].index.values)
  return df.drop(exclude_indices, axis='index')

df8 = remove_bhk_outliers(df7)
df8.shape

scatter_plot(df8, 'Rajaji Nagar')

plt.rcParams['figure.figsize']=(20,10)
plt.hist(df8.price_per_sqft, rwidth=0.8)
plt.xlabel("Price per square feet")
plt.ylabel("Count")

"""Exploring bathroom features"""

df8[df8.bath>10]

plt.hist(df8.bath, rwidth=0.8)
plt.xlabel("Number of bathrooms")
plt.ylabel("Count")

# outliers as an example 4 BHK has 7 bathroom which is not good
df8[df8.bath>df8.bhk+2]

df9 = df8[df8.bath<df8.bhk+2]
df9.shape

"""Finally we can drop **price_per_sqft(used only for outlier detection)** and **size**"""

df10 = df9.drop(["price_per_sqft", "size"], axis=1)
df10.head()

"""## **Creating our best fit model**"""

# handling dummy variables

dummies = pd.get_dummies(df10.location, drop_first=True)
dummies.head()

final_df = pd.concat([df10, dummies], axis=1)
final_df.drop('location', axis=1, inplace=True)
final_df.head()

final_df.shape

# Spearating X(independent) and y(dependent) variables

X = final_df.drop('price', axis=1)
X.head()

y = final_df.price
y.head()

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

from sklearn.linear_model import LinearRegression

lreg = LinearRegression()
lreg.fit(X_train, y_train)
lreg.score(X_test, y_test)

"""**Shuffle Split** is used so that in each fold in cross validation, there is an equal distribution"""

# Using KFold cross validation and shuffle split 

from sklearn.model_selection import ShuffleSplit
from sklearn.model_selection import cross_val_score

cv = ShuffleSplit(n_splits=5, test_size=0.2, random_state=0)

cross_val_score(LinearRegression(),X, y, cv=cv)

# Finding score for various Regression models

from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import ShuffleSplit

from sklearn.linear_model import LinearRegression
from sklearn.linear_model import Lasso
from sklearn.tree import DecisionTreeRegressor

def find_best_reg_model(X, y):
  algos = {
      'lreg' : {
          'model': LinearRegression(),
          'params': {
              'normalize': [True, False]
          }
      }, 
      'lareg': {
          'model': Lasso(),
          'params': {
              'alpha': [1,2],
              'selection': ['random', 'cyclic']
          }
      },
      'd_tree': {
          'model': DecisionTreeRegressor(),
          'params': {
              'criterion': ['mse', 'friedman_mse'],
              'splitter': ['best', 'random']
          }
      },
  }

  scores = []
  cv=ShuffleSplit(n_splits=5, test_size=0.2, random_state=0)
  for model_name, mp in algos.items():
    reg = GridSearchCV(mp["model"], mp["params"], cv=cv, return_train_score=False)
    reg.fit(X,y)
    scores.append({
        'model': model_name,
        'best_score': reg.best_score_,
        'best_params': reg.best_params_
    })

  return pd.DataFrame(scores)

find_best_reg_model(X, y)

lreg = LinearRegression(normalize=False)
lreg.fit(X, y)
def predict_price(location, sqft, bath, bhk):
  loc_index = np.where(X.columns==location)[0][0]

  x = np.zeros(len(X.columns))
  x[0] = sqft
  x[1] = bath
  x[2] = bhk

  if loc_index>=0:
    x[loc_index] = 1
  return lreg.predict([x])[0]

predict_price('1st Phase JP Nagar', 1000, 2, 2)

predict_price('1st Phase JP Nagar', 1000, 2, 3)

predict_price('Indira Nagar', 1000, 2, 2)

predict_price('Indira Nagar', 1700, 2, 3)

import pickle
with open("home_price_model.pickle", 'wb') as f:
  pickle.dump(lreg, f)

# importing our final_df's column's info into a JSON file
import json
columns = {
    'data_columns': [col.lower() for col in X.columns]
}
with open('columns.json', 'w') as f:
  f.write(json.dumps(columns))